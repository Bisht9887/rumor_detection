{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# importing all the required libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"           \\nif __name__ == '__main__':\\n    average_length_posts_of_user()\\n    positive_negative_words()\\n    microblogs_with_url()\\n    microblogs_with_emoticons()\\n    positive_negaive_microblogs()\\n    first_person_pronouns()\\n    microblogs_with_hashtags()\\n    microblogs_with_at()\\n    microblogs_with_question()\\n    microblogs_with_exclamation()\\n    microblogs_with_question_exclamation()\\n    verified_users()\\n    users_friends()\\n    users_followers()\\n    users_posts()\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# opening and parsing all the json files \n",
    "\n",
    "dataset_list=[] # it contains all the data. All the data extracted for all the features for every event(JSON files) would be added to it\n",
    "for filename in glob.glob('*.json'):\n",
    "    filelist=[] # contains data of every feature for a single event(json file) at a time\n",
    "    with open(filename, encoding=\"utf8\") as json_data:\n",
    "        \n",
    "        data = json.load(json_data)\n",
    "        \n",
    "        list_publish_time=[] # list for time at which a tweet was published.\n",
    "        List_unsorted_users=[]  # list for % of verified users in an interval\n",
    "        List_unsorted_friends=[] # list for Average # of friends of users in an interval\n",
    "        List_unsorted_followers=[] # list Average # of followers of users in an interval\n",
    "        List_unsorted_posts=[] # List for Average # of posts of users in an interval\n",
    "        list_tweet_length=[] # List contaning the length of tweet\n",
    "        list_tweet_detail=[] # list contaning tweets  \n",
    "        # all the above lists are unsorted\n",
    "        \n",
    "        i=0 # position of the data we are extracting\n",
    "        for r in data['TweetsList']:\n",
    "            \n",
    "            #Appedind data\n",
    "            line= data[\"TweetsList\"][i][\"pubTime\"]\n",
    "            date_pattern = re.compile(r'[0-9]+:[0-9]+ [a-zA-Z]+ - [0-9]+ [a-zA-Z]+ [0-9]+',re.I)\n",
    "            matches_date = date_pattern.findall(line)\n",
    "            for match in matches_date:\n",
    "                date_time= datetime.strptime(match, '%I:%M %p - %d %b %Y')\n",
    "                list_publish_time.append(date_time)\n",
    "\n",
    "            #Appedind data\n",
    "            user_tweets_1= data[\"TweetsList\"][i][\"userObj\"][\"verified\"]        \n",
    "            List_unsorted_users.append(user_tweets_1)\n",
    "            \n",
    "            #Appedind data\n",
    "            user_tweets_2= data[\"TweetsList\"][i][\"userObj\"][\"numFriends\"]\n",
    "            List_unsorted_friends.append(user_tweets_2)\n",
    "\n",
    "            #Appedind data\n",
    "            user_tweets_3= data[\"TweetsList\"][i][\"userObj\"][\"numFollowers\"]\n",
    "            List_unsorted_followers.append(user_tweets_3)\n",
    "\n",
    "            #Appedind data\n",
    "            user_tweets_4= data[\"TweetsList\"][i][\"userObj\"][\"numTweets\"]\n",
    "            List_unsorted_posts.append(user_tweets_4)\n",
    "\n",
    "            #Appedind data\n",
    "            user_tweets_5= data[\"TweetsList\"][i][\"detail\"]\n",
    "            length_t = len(user_tweets_5.split())\n",
    "            list_tweet_length.append(length_t)\n",
    "\n",
    "            #Appedind data\n",
    "            detailofjson = user_tweets_5        \n",
    "            list_tweet_detail.append(detailofjson)\n",
    "\n",
    "            i+=1\n",
    "    \n",
    "    # sorting the lists\n",
    "    sorted_List_time = list(list_publish_time)\n",
    "    \n",
    "    sorted_List_time.sort()\n",
    "    sorted_list_tweet_length = [p for _,p in sorted(zip(list_publish_time,list_tweet_length))]\n",
    "    sorted_list_tweet_detail = [p for _,p in sorted(zip(list_publish_time,list_tweet_detail))]\n",
    "    List_sorted_users = [p for _,p in sorted(zip(list_publish_time,List_unsorted_users))]\n",
    "    List_sorted_friends = [p for _,p in sorted(zip(list_publish_time,List_unsorted_friends))]\n",
    "    List_sorted_followers = [p for _,p in sorted(zip(list_publish_time,List_unsorted_followers))]\n",
    "    List_sorted_posts = [p for _,p in sorted(zip(list_publish_time,List_unsorted_posts))]\n",
    "    # All the above lists are now sorted\n",
    "    \n",
    "    import math\n",
    "    interval_duration = math.ceil(abs((sorted_List_time[-1] - sorted_List_time[0]).total_seconds()/3600.0)/50.0) # length of each interval for each event\n",
    "   \n",
    "    #**********************************IMPLEMENTING EACH FEATURE********************************\n",
    "    \n",
    "    #*****Average length of microblogs*****#                               \n",
    "    \n",
    "    ftilda_avg_microblogs_length=[]  # all the f_tilda...[] lists contain f ~(tilda) values which is equation (6) \n",
    "                                        #f~(t,k) = (f (t,k) − fk)/ σ(fk)\n",
    "\n",
    "    def average_length_posts_of_user(): \n",
    "\n",
    "        list_avg_microblogs_length = [] # contains the average of length of tweets(posts) made in each interval or the Average length of microblogs\n",
    "\n",
    "        begin_time=sorted_List_time[0]\n",
    "        tweet_number=0\n",
    "        tweet_info=0\n",
    "        interval=1\n",
    "        for i in range (1,51): # because there are total 50 intervals\n",
    "\n",
    "            end_time = begin_time+timedelta(hours=interval_duration)\n",
    "            if (tweet_info<len(sorted_List_time) and sorted_List_time[tweet_info]<=end_time):\n",
    "                #print('Interval',interval, ':')\n",
    "                count=0\n",
    "                sum_=0\n",
    "                while (begin_time <= end_time and tweet_info<len(sorted_List_time)):\n",
    "                    if (sorted_List_time[tweet_info]<=end_time):\n",
    "                        tweet_number +=1\n",
    "                        count +=1\n",
    "                        sum_= sum_ + (sorted_list_tweet_length[tweet_info])\n",
    "                        tweet_info+=1\n",
    "\n",
    "                    begin_time += timedelta(hours=1)\n",
    "\n",
    "\n",
    "                avg_microblog_length=round(sum_/count,2)\n",
    "                #print (\"Average length of microblogs in interval\",interval,\":\",avg_microblog_length)\n",
    "                list_avg_microblogs_length.append(avg_microblog_length)\n",
    "                #print (\"\\n\")\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "            else:\n",
    "                #print('Interval',interval, ': No tweets')\n",
    "                list_avg_microblogs_length.append(0)\n",
    "                end_time += timedelta(hours=1)\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "\n",
    "        std_dev=np.std(list_avg_microblogs_length) # standard deviation\n",
    "        mean_f=np.mean(list_avg_microblogs_length) # mean\n",
    "        length_f = len(list_avg_microblogs_length)\n",
    "\n",
    "        for i in range(length_f):\n",
    "\n",
    "            f_tilda= (list_avg_microblogs_length[i]- mean_f)/std_dev\n",
    "            ftilda_avg_microblogs_length.append(f_tilda)\n",
    "    average_length_posts_of_user()       \n",
    "    filelist.extend(ftilda_avg_microblogs_length)\n",
    "    \n",
    "    #****************number of positive (negative) words in microblogs**********#\n",
    "    \n",
    "    from collections import Counter # to count the number of words\n",
    "\n",
    "\n",
    "    ftilda_positive_negative_words=[]\n",
    "    def positive_negative_words():\n",
    "\n",
    "\n",
    "        list_positive_negative_words=[]\n",
    "        def readwords( filename ):\n",
    "            f = open(filename)\n",
    "            words = [ line.rstrip() for line in f.readlines()]\n",
    "            return words\n",
    "\n",
    "        positive = readwords('positive.txt') # this file contains most of the positive words of the English dictionary\n",
    "        negative = readwords('negative.txt') # this file contains most of the negative words of the English dictionary\n",
    "\n",
    "        begin_time= sorted_List_time[0]\n",
    "\n",
    "        tweet_number= 0\n",
    "        tweet_info= 0\n",
    "        interval= 1\n",
    "        for i in range (1,51):\n",
    "\n",
    "            end_time = begin_time+timedelta(hours=interval_duration)\n",
    "            total_words=0\n",
    "\n",
    "            if (tweet_info<len(sorted_List_time) and sorted_List_time[tweet_info]<=end_time):\n",
    "                #print('Interval',interval, ':')\n",
    "\n",
    "                total_microblogs=0\n",
    "\n",
    "                while (begin_time <= end_time and tweet_info<len(sorted_List_time)):\n",
    "\n",
    "\n",
    "                    if (sorted_List_time[tweet_info]<=end_time):\n",
    "                        tweet_number +=1\n",
    "                        total_microblogs +=1\n",
    "                        tweet_detail= sorted_list_tweet_detail[tweet_info]\n",
    "                        count = Counter(tweet_detail.split())\n",
    "\n",
    "                        pos = 0\n",
    "                        neg = 0\n",
    "                        for key, val in count.items():\n",
    "                            key = key.rstrip('.,?!\\n') # removing possible punctuation signs\n",
    "                            if key in positive:\n",
    "                                pos += val\n",
    "\n",
    "                            if key in negative:\n",
    "                                neg += val\n",
    "                        total_words = total_words + neg + pos\n",
    "\n",
    "                        tweet_info+=1\n",
    "                    begin_time += timedelta(hours=1)\n",
    "\n",
    "\n",
    "                #print (\"total_positive_negative_words\",total_words)\n",
    "                #print (\"\\n\")\n",
    "                list_positive_negative_words.append(total_words)\n",
    "                begin_time = end_time\n",
    "                interval +=1\n",
    "            else:\n",
    "                #print('Interval',interval, ': No tweets')\n",
    "                list_positive_negative_words.append(0)\n",
    "                end_time += timedelta(hours=1)\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "        std_dev=np.std(list_positive_negative_words)\n",
    "        mean_f=np.mean(list_positive_negative_words)\n",
    "        length_f = len(list_positive_negative_words)\n",
    "        for i in range(length_f):\n",
    "            f_tilda= (list_positive_negative_words[i]- mean_f)/std_dev\n",
    "            ftilda_positive_negative_words.append(f_tilda)\n",
    "    positive_negative_words()\n",
    "    filelist.extend(ftilda_positive_negative_words)\n",
    "    \n",
    "  #***************** % of microblogs with URL*****************###   \n",
    "    \n",
    "    ftilda_microblosg_url=[]\n",
    "    def microblogs_with_url():\n",
    "        list_microblogs_url = []\n",
    "\n",
    "\n",
    "        begin_time=sorted_List_time[0]\n",
    "\n",
    "        tweet_number= 0\n",
    "        tweet_info= 0\n",
    "        interval= 1\n",
    "        for i in range (1,51):\n",
    "\n",
    "            end_time = begin_time+timedelta(hours=interval_duration)\n",
    "\n",
    "            if (tweet_info<len(sorted_List_time) and sorted_List_time[tweet_info]<=end_time):\n",
    "                #print('Interval',interval, ':')\n",
    "                microblogs_wit_url = 0\n",
    "                total_microblogs=0\n",
    "\n",
    "                while (begin_time <= end_time and tweet_info<len(sorted_List_time)):\n",
    "                    if (sorted_List_time[tweet_info]<=end_time):\n",
    "                        tweet_number +=1\n",
    "                        total_microblogs +=1\n",
    "                        tweet_detail= sorted_list_tweet_detail[tweet_info]\n",
    "                        required_data = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', tweet_detail)\n",
    "\n",
    "                        if required_data:\n",
    "                            microblogs_wit_url += 1\n",
    "                        tweet_info+=1\n",
    "                    begin_time += timedelta(hours=1)\n",
    "\n",
    "                percent = microblogs_wit_url/total_microblogs\n",
    "                percentage_microblog_url= round(percent * 100,2)\n",
    "                #print(\"percentage of microblogs with URL:\",percentage_microblog_url,\"%\")\n",
    "                list_microblogs_url.append(percentage_microblog_url)\n",
    "                #print (\"\\n\")\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "            else:\n",
    "                #print('Interval',interval, ': No tweets')\n",
    "                list_microblogs_url.append(0)\n",
    "                end_time += timedelta(hours=1)\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "        std_dev=np.std(list_microblogs_url)\n",
    "        mean_f=np.mean(list_microblogs_url)\n",
    "        length_f = len(list_microblogs_url)\n",
    "        for i in range(length_f):\n",
    "            f_tilda= (list_microblogs_url[i]- mean_f)/std_dev\n",
    "            ftilda_microblosg_url.append(f_tilda)\n",
    "    microblogs_with_url()\n",
    "    filelist.extend(ftilda_microblosg_url)\n",
    "    \n",
    " ##**************** % of microblogs with smiling (frowning) emoticons *********************##\n",
    "            \n",
    "    ftilda_microblogs_emoticons=[]\n",
    "    def microblogs_with_emoticons():\n",
    "        list_microblogs_emoticons=[]\n",
    "        begin_time=sorted_List_time[0]\n",
    "\n",
    "        tweet_number=0\n",
    "        tweet_info=0\n",
    "        interval =1\n",
    "        for i in range (1,51):\n",
    "\n",
    "            end_time = begin_time+timedelta(hours=interval_duration)\n",
    "\n",
    "            if (tweet_info<len(sorted_List_time) and sorted_List_time[tweet_info]<=end_time):\n",
    "                #print('Interval',interval, ':')\n",
    "                microblogs_wit_emoticons = 0\n",
    "                total_microblogs=0\n",
    "\n",
    "                while (begin_time <= end_time and tweet_info<len(sorted_List_time)):\n",
    "                    #print('before if z=',z)\n",
    "\n",
    "                    if (sorted_List_time[tweet_info]<=end_time):\n",
    "                        tweet_number +=1\n",
    "                        total_microblogs +=1\n",
    "                        tweet_detail= sorted_list_tweet_detail[tweet_info]\n",
    "                        required_data = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',tweet_detail)\n",
    "\n",
    "                        if required_data:\n",
    "                            #print(tweet_detail)\n",
    "                            microblogs_wit_emoticons+=1\n",
    "                        tweet_info+=1\n",
    "                    begin_time += timedelta(hours=1)\n",
    "\n",
    "\n",
    "                percent = microblogs_wit_emoticons/total_microblogs\n",
    "                percentage_of_emoticons = round(percent * 100,2)\n",
    "\n",
    "                #print(\"percentage of microblogs with  smiling(frowning) emoticons:\",percentage_of_emoticons,\"%\")\n",
    "                list_microblogs_emoticons.append(percentage_of_emoticons)\n",
    "                #print (\"\\n\")\n",
    "                begin_time = end_time\n",
    "                interval += 1\n",
    "            else:\n",
    "                #print('Interval',interval, ': No tweets')\n",
    "                list_microblogs_emoticons.append(0)\n",
    "                end_time += timedelta(hours=1)\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "        std_dev=np.std(list_microblogs_emoticons)\n",
    "        mean_f=np.mean(list_microblogs_emoticons)\n",
    "        length_f = len(list_microblogs_emoticons)\n",
    "        for i in range(length_f):\n",
    "            f_tilda= (list_microblogs_emoticons[i]- mean_f)/std_dev\n",
    "            ftilda_microblogs_emoticons.append(f_tilda)\n",
    "    microblogs_with_emoticons()\n",
    "    filelist.extend(ftilda_microblogs_emoticons)\n",
    "    \n",
    "    \n",
    " #######************* % of positive (negative) microblogs**********************###########\n",
    "    from textblob import TextBlob\n",
    "    ftilda_positive_negaive_microblogs=[]\n",
    "    def positive_negaive_microblogs():\n",
    "        list_positive_negaive_microblogs = []\n",
    "        begin_time=sorted_List_time[0]\n",
    "\n",
    "        tweet_number=0\n",
    "        tweet_info=0\n",
    "        interval =1\n",
    "        for i in range (1,51):\n",
    "\n",
    "            end_time = begin_time+timedelta(hours=interval_duration)\n",
    "            total_positive_negative = 0\n",
    "            if (tweet_info <len(sorted_List_time) and sorted_List_time[tweet_info]<=end_time):\n",
    "                #print('Interval',interval, ':')\n",
    "                negative_microblog_count = 0\n",
    "                positive_microblog_count = 0\n",
    "                total_microblogs=0\n",
    "\n",
    "                while (begin_time <= end_time and tweet_info<len(sorted_List_time)):\n",
    "                    #print('before if z=',z)\n",
    "\n",
    "                    if (sorted_List_time[tweet_info]<=end_time):\n",
    "                        tweet_number +=1\n",
    "                        total_microblogs +=1\n",
    "                        tweet_detail= sorted_list_tweet_detail[tweet_info]\n",
    "                        required_data = TextBlob(tweet_detail)\n",
    "                        if ((required_data.sentiment.polarity)>0):\n",
    "                            #print (tweet_detail)\n",
    "                            positive_microblog_count += 1\n",
    "                        elif ((required_data.sentiment.polarity)<0):\n",
    "                            negative_microblog_count += 1\n",
    "                        tweet_info+=1\n",
    "\n",
    "                    begin_time += timedelta(hours=1)\n",
    "\n",
    "                positive_percent = positive_microblog_count/total_microblogs\n",
    "                negative_percent = negative_microblog_count/total_microblogs\n",
    "                total_positive_negative = positive_microblog_count + negative_microblog_count\n",
    "                #print(\"total number of microblogs:\",total_microblogs)\n",
    "                #print(\"number of positive microblogs:\",positive_microblog_count)\n",
    "                #print(\"number of negative microblogs:\",negative_microblog_count)\n",
    "                #print (\"total count:\", total_positive_negative )\n",
    "                #print(\"percentage of positive microblogs:\",round(positive_percent * 100,2),\"%\")\n",
    "                #print(\"percentage of negative microblogs:\",round(negative_percent * 100,2),\"%\")\n",
    "                list_positive_negaive_microblogs.append(total_positive_negative)\n",
    "                #print (\"\\n\")\n",
    "                begin_time = end_time\n",
    "                interval +=1\n",
    "            else:\n",
    "                #print('Interval',interval, ': No tweets')\n",
    "                list_positive_negaive_microblogs.append(0)\n",
    "                end_time += timedelta(hours=1)\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "        std_dev=np.std(list_positive_negaive_microblogs)\n",
    "        mean_f=np.mean(list_positive_negaive_microblogs)\n",
    "        length_f = len(list_positive_negaive_microblogs)\n",
    "        for i in range(length_f):\n",
    "            f_tilda= (list_positive_negaive_microblogs[i]- mean_f)/std_dev\n",
    "            ftilda_positive_negaive_microblogs.append(f_tilda)\n",
    "    \n",
    "    positive_negaive_microblogs()\n",
    "    filelist.extend(ftilda_positive_negaive_microblogs)\n",
    "    \n",
    " #####************% of microblogs with the first-person pronouns***********************#########\n",
    "    ftilda_first_person_pronouns=[]\n",
    "    def first_person_pronouns():\n",
    "        list_first_person_pronouns = []\n",
    "        begin_time=sorted_List_time[0]\n",
    "\n",
    "        tweet_number=0\n",
    "        tweet_info=0\n",
    "        interval =1\n",
    "        for i in range (1,51):\n",
    "\n",
    "            end_time = begin_time+timedelta(hours=interval_duration)\n",
    "\n",
    "            if (tweet_info<len(sorted_List_time) and sorted_List_time[tweet_info]<=end_time):\n",
    "                #print('Interval',interval, ':')\n",
    "                first_person_microblogs = 0\n",
    "                total_microblogs=0\n",
    "\n",
    "                while (begin_time <= end_time and tweet_info<len(sorted_List_time)):\n",
    "                    #print('before if z=',z)\n",
    "\n",
    "                    if (sorted_List_time[tweet_info]<=end_time):\n",
    "                        tweet_number +=1\n",
    "                        total_microblogs +=1\n",
    "                        tweet_detail= sorted_list_tweet_detail[tweet_info]\n",
    "                        # I don't think the below method is an optimal one because we may miss to include some pronouns \n",
    "                        list_ = ['I', 'i','me','me,','ME','we', 'my', 'mine', 'myself', 'us', 'our', 'ours', 'ourselves']\n",
    "                        word_set = set(list_)\n",
    "                        #key=tweet_detail.rstrip('.,?!\\n') \n",
    "                        phrase_set = set(tweet_detail.split())\n",
    "                        if word_set.intersection(phrase_set):\n",
    "                            first_person_microblogs+=1\n",
    "                        tweet_info+=1\n",
    "                    begin_time += timedelta(hours=1)\n",
    "\n",
    "\n",
    "                percent = first_person_microblogs/total_microblogs\n",
    "                #print(\"microblogs with first-person pronouns:\",first_person_microblogs)\n",
    "                #print(\"total number of microblogs:\",total_microblogs)\n",
    "                percentage = round(percent * 100,2)\n",
    "                #print(\"percentage of microblogs with first-person pronouns :\",percentage,\"%\")\n",
    "                list_first_person_pronouns.append(percentage)\n",
    "                #print (\"\\n\")\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "            else:\n",
    "                #print('Interval',interval, ': No tweets')\n",
    "                list_first_person_pronouns.append(0)\n",
    "                end_time += timedelta(hours=1)\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "        std_dev=np.std(list_first_person_pronouns)\n",
    "        mean_f=np.mean(list_first_person_pronouns)\n",
    "        length_f = len(list_first_person_pronouns)\n",
    "        for i in range(length_f):\n",
    "            f_tilda= (list_first_person_pronouns[i]- mean_f)/std_dev\n",
    "            ftilda_first_person_pronouns.append(f_tilda)\n",
    "    \n",
    "    first_person_pronouns()\n",
    "    filelist.extend(ftilda_first_person_pronouns)\n",
    "    \n",
    " ###************ % of microblogs with hashtags*********************########\n",
    "\n",
    "    ftilda_microblogs_with_hashtags=[]\n",
    "    def microblogs_with_hashtags():\n",
    "        list_microblogs_with_hashtags = []\n",
    "        begin_time=sorted_List_time[0]\n",
    "\n",
    "        tweet_number=0\n",
    "        tweet_info=0\n",
    "        interval =1\n",
    "        for i in range (1,51):\n",
    "\n",
    "            end_time = begin_time+timedelta(hours=interval_duration)\n",
    "\n",
    "            if (tweet_info<len(sorted_List_time) and sorted_List_time[tweet_info]<=end_time):\n",
    "                #print('Interval',interval, ':')\n",
    "                microblogs_wit_hashtags = 0\n",
    "                total_microblogs=0\n",
    "\n",
    "                while (begin_time <= end_time and tweet_info<len(sorted_List_time)):\n",
    "\n",
    "                    if (sorted_List_time[tweet_info]<=end_time):\n",
    "                        tweet_number +=1\n",
    "                        total_microblogs +=1\n",
    "                        tweet_detail= sorted_list_tweet_detail[tweet_info]\n",
    "                        required_data = re.findall(r\"#(\\w+)\", tweet_detail)\n",
    "\n",
    "                        if required_data:\n",
    "                            #print(tweet_detail)\n",
    "                            microblogs_wit_hashtags+=1\n",
    "                        tweet_info+=1\n",
    "                    begin_time += timedelta(hours=1)\n",
    "\n",
    "\n",
    "                percent = microblogs_wit_hashtags/total_microblogs\n",
    "                #print(\"microblogs with hashtags:\",microblogs_wit_hashtags)\n",
    "                #print(\"total number of microblogs:\",total_microblogs)\n",
    "                percentage=round(percent * 100,2)\n",
    "                #print(\"percentage of microblogs with hashtags:\",percentage,\"%\")\n",
    "                list_microblogs_with_hashtags.append(percentage)\n",
    "                #print (\"\\n\")\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "            else:\n",
    "                #print('Interval',interval, ': No tweets')\n",
    "                list_microblogs_with_hashtags.append(0)\n",
    "                end_time += timedelta(hours=1)\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "        std_dev=np.std(list_microblogs_with_hashtags)\n",
    "        mean_f=np.mean(list_microblogs_with_hashtags)\n",
    "        length_f = len(list_microblogs_with_hashtags)\n",
    "        for i in range(length_f):\n",
    "            f_tilda= (list_microblogs_with_hashtags[i]- mean_f)/std_dev\n",
    "            ftilda_microblogs_with_hashtags.append(f_tilda)\n",
    " \n",
    "    microblogs_with_hashtags()\n",
    "    filelist.extend(ftilda_microblogs_with_hashtags)\n",
    "    \n",
    " ###************ % of microblogs with @ mentions *****************####\n",
    "    ftilda_microblogs_with_at=[]\n",
    "    def microblogs_with_at():\n",
    "        list_microblogs_with_at=[]\n",
    "        begin_time=sorted_List_time[0]\n",
    "\n",
    "        tweet_number=0\n",
    "        tweet_info=0\n",
    "        interval =1\n",
    "        for i in range (1,51):\n",
    "\n",
    "            end_time = begin_time+timedelta(hours=interval_duration)\n",
    "\n",
    "            if (tweet_info<len(sorted_List_time) and sorted_List_time[tweet_info]<=end_time):\n",
    "                #print('Interval',interval, ':')\n",
    "                microblogs_wit_at = 0\n",
    "                total_microblogs=0\n",
    "\n",
    "                while (begin_time <= end_time and tweet_info<len(sorted_List_time)):\n",
    "                    #print('before if z=',z)\n",
    "\n",
    "                    if (sorted_List_time[tweet_info]<=end_time):\n",
    "                        tweet_number +=1\n",
    "                        total_microblogs +=1\n",
    "                        tweet_detail= sorted_list_tweet_detail[tweet_info]\n",
    "                        required_data = re.findall(r\"@(\\w+)\", tweet_detail)\n",
    "\n",
    "                        if required_data:\n",
    "                            #print(tweet_detail)\n",
    "                            microblogs_wit_at+=1\n",
    "                        tweet_info+=1\n",
    "                    begin_time += timedelta(hours=1)\n",
    "\n",
    "\n",
    "                percent = microblogs_wit_at/total_microblogs\n",
    "                #print(\"microblogs with @:\",microblogs_wit_at)\n",
    "                #print(\"total number of microblogs:\",total_microblogs)\n",
    "                percentage=round(percent * 100,2)\n",
    "                #print(\"percentage of microblogs with @:\",percentage,\"%\")\n",
    "                #print (\"\\n\")\n",
    "                list_microblogs_with_at.append(percentage)\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "            else:\n",
    "                #print('Interval',interval, ': No tweets')\n",
    "                list_microblogs_with_at.append(0)\n",
    "                end_time += timedelta(hours=1)\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "\n",
    "        std_dev=np.std(list_microblogs_with_at)\n",
    "        mean_f=np.mean(list_microblogs_with_at)\n",
    "        length_f = len(list_microblogs_with_at)\n",
    "        for i in range(length_f):\n",
    "            f_tilda= (list_microblogs_with_at[i]- mean_f)/std_dev\n",
    "            ftilda_microblogs_with_at.append(f_tilda)\n",
    "\n",
    "    microblogs_with_at()\n",
    "    filelist.extend(ftilda_microblogs_with_at)\n",
    "    \n",
    " ####************ % of microblogs with question marks ****************######    \n",
    "    ftilda_microblogs_with_question=[]\n",
    "    def microblogs_with_question():\n",
    "        list_microblogs_with_question = []\n",
    "        begin_time=sorted_List_time[0]\n",
    "\n",
    "        tweet_number=0\n",
    "        tweet_info=0\n",
    "        interval =1\n",
    "        for i in range (1,51):\n",
    "\n",
    "            end_time = begin_time+timedelta(hours=interval_duration)\n",
    "\n",
    "            if (tweet_info<len(sorted_List_time) and sorted_List_time[tweet_info]<=end_time):\n",
    "                #print('Interval',interval, ':')\n",
    "                microblogs_wit_question = 0\n",
    "                total_microblogs=0\n",
    "\n",
    "                while (begin_time <= end_time and tweet_info<len(sorted_List_time)):\n",
    "                    #print('before if z=',z)\n",
    "\n",
    "                    if (sorted_List_time[tweet_info]<=end_time):\n",
    "                        tweet_number +=1\n",
    "                        total_microblogs +=1\n",
    "                        tweet_detail= sorted_list_tweet_detail[tweet_info]\n",
    "                        required_data = '?' in tweet_detail[:-1]\n",
    "\n",
    "                        if required_data:\n",
    "                            #print(tweet_detail)\n",
    "                            microblogs_wit_question+=1\n",
    "                        tweet_info+=1\n",
    "                    begin_time += timedelta(hours=1)\n",
    "\n",
    "\n",
    "                percent = microblogs_wit_question/total_microblogs\n",
    "                #print(\"microblogs with question marks:\",microblogs_wit_question)\n",
    "                #print(\"total number of microblogs:\",total_microblogs)\n",
    "                percentage = round(percent * 100,2)\n",
    "                #print(\"percentage of microblogs with question marks:\", percentage ,\"%\")\n",
    "                #print (\"\\n\")\n",
    "                list_microblogs_with_question.append(percentage)\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "            else:\n",
    "                #print('Interval',interval, ': No tweets')\n",
    "                list_microblogs_with_question.append(0)\n",
    "                end_time += timedelta(hours=1)\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "        std_dev=np.std(list_microblogs_with_question)\n",
    "        mean_f=np.mean(list_microblogs_with_question)\n",
    "        length_f = len(list_microblogs_with_question)\n",
    "        for i in range(length_f):\n",
    "            f_tilda= (list_microblogs_with_question[i]- mean_f)/std_dev\n",
    "            ftilda_microblogs_with_question.append(f_tilda)\n",
    "            \n",
    "    microblogs_with_question()\n",
    "    filelist.extend(ftilda_microblogs_with_question)\n",
    "  ####********** % of microblogs with exclamation marks *************************###   \n",
    "    ftilda_microblogs_with_exclamation=[]\n",
    "    def microblogs_with_exclamation():\n",
    "        list_microblogs_with_exclamation = []\n",
    "        begin_time=sorted_List_time[0]\n",
    "\n",
    "        tweet_number=0\n",
    "        tweet_info=0\n",
    "        interval =1\n",
    "        for i in range (1,51):\n",
    "\n",
    "            end_time = begin_time+timedelta(hours=interval_duration)\n",
    "\n",
    "            if (tweet_info<len(sorted_List_time) and sorted_List_time[tweet_info]<=end_time):\n",
    "                #print('Interval',interval, ':')\n",
    "                microblogs_wit_exclamation = 0\n",
    "                total_microblogs=0\n",
    "\n",
    "                while (begin_time <= end_time and tweet_info<len(sorted_List_time)):\n",
    "                    #print('before if z=',z)\n",
    "\n",
    "                    if (sorted_List_time[tweet_info]<=end_time):\n",
    "                        tweet_number +=1\n",
    "                        total_microblogs +=1\n",
    "                        tweet_detail= sorted_list_tweet_detail[tweet_info]\n",
    "                        #print(tweet_detail)\n",
    "                        required_data = '!' in tweet_detail[:-1]\n",
    "\n",
    "                        if required_data:\n",
    "                            #print(tweet_detail)\n",
    "                            microblogs_wit_exclamation+=1\n",
    "                        tweet_info+=1\n",
    "                    begin_time += timedelta(hours=1)\n",
    "\n",
    "\n",
    "                percent = microblogs_wit_exclamation/total_microblogs\n",
    "                #print(\"microblogs with exclamation marks:\",microblogs_wit_exclamation)\n",
    "                #print(\"total number of microblogs:\",total_microblogs)\n",
    "                percentage = round(percent * 100,2)\n",
    "                #print(\"percentage of microblogs with exclamation marks:\",percentage,\"%\")\n",
    "                #print (\"\\n\")\n",
    "                list_microblogs_with_exclamation.append(percentage)\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "            else:\n",
    "                #print('Interval',interval, ': No tweets')\n",
    "                list_microblogs_with_exclamation.append(0)\n",
    "                end_time += timedelta(hours=1)\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "        std_dev=np.std(list_microblogs_with_exclamation)\n",
    "        mean_f=np.mean(list_microblogs_with_exclamation)\n",
    "        length_f = len(list_microblogs_with_exclamation)\n",
    "        for i in range(length_f):\n",
    "            f_tilda= (list_microblogs_with_exclamation[i]- mean_f)/std_dev\n",
    "            ftilda_microblogs_with_exclamation.append(f_tilda)\n",
    "            \n",
    "    microblogs_with_exclamation()\n",
    "    filelist.extend(ftilda_microblogs_with_exclamation)\n",
    "    \n",
    "  #####*******  % of microblogs with multiple question/exclamation marks *****************###   \n",
    "    \n",
    "    ftilda_microblogs_with_question_exclamation=[]\n",
    "\n",
    "    def microblogs_with_question_exclamation():\n",
    "        list_microblogs_with_question_exclamation= []\n",
    "        begin_time=sorted_List_time[0]\n",
    "\n",
    "        tweet_number=0\n",
    "        tweet_info=0\n",
    "        interval =1\n",
    "        for i in range (1,51):\n",
    "\n",
    "            end_time = begin_time+timedelta(hours=interval_duration)\n",
    "\n",
    "            if (tweet_info<len(sorted_List_time) and sorted_List_time[tweet_info]<=end_time):\n",
    "                #print('Interval',interval, ':')\n",
    "                microblogs_wit_question_exclamation = 0\n",
    "                total_microblogs=0\n",
    "\n",
    "                while (begin_time <= end_time and tweet_info<len(sorted_List_time)):\n",
    "                    #print('before if z=',z)\n",
    "\n",
    "                    if (sorted_List_time[tweet_info]<=end_time):\n",
    "                        tweet_number +=1\n",
    "                        total_microblogs +=1\n",
    "                        tweet_detail= sorted_list_tweet_detail[tweet_info]\n",
    "                        required_data_1 = tweet_detail.count('?')\n",
    "                        required_data_2 = tweet_detail.count('!')\n",
    "\n",
    "                        if required_data_1 > 1 or required_data_2 > 1:\n",
    "                            #print(tweet_detail)\n",
    "                            microblogs_wit_question_exclamation+=1\n",
    "                        tweet_info+=1\n",
    "                    begin_time += timedelta(hours=1)\n",
    "\n",
    "                #print (\"Average length of microblogs in interval\",t,\":\",sum/count)\n",
    "                percent = microblogs_wit_question_exclamation/total_microblogs\n",
    "                #print(\"microblogs with multiple question/exclamation marks:\",microblogs_wit_question_exclamation)\n",
    "                #print(\"total number of microblogs:\",total_microblogs)\n",
    "                percentage = round(percent * 100,2)\n",
    "                #print(\"percentage of microblogs with questions/exclamation marks:\",percentage ,\"%\")\n",
    "                #print (\"\\n\")\n",
    "                list_microblogs_with_question_exclamation.append(percentage)\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "            else:\n",
    "                #print('Interval',interval, ': No tweets')\n",
    "                list_microblogs_with_question_exclamation.append(0)\n",
    "                end_time += timedelta(hours=1)\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "        std_dev=np.std(list_microblogs_with_question_exclamation)\n",
    "        mean_f=np.mean(list_microblogs_with_question_exclamation)\n",
    "        length_f = len(list_microblogs_with_question_exclamation)\n",
    "        for i in range(length_f):\n",
    "            f_tilda= (list_microblogs_with_question_exclamation[i]- mean_f)/std_dev\n",
    "            ftilda_microblogs_with_question_exclamation.append(f_tilda)\n",
    "   \n",
    "    microblogs_with_question_exclamation()\n",
    "    filelist.extend(ftilda_microblogs_with_question_exclamation)\n",
    "    \n",
    "  ######************ % of verified users *****************####   \n",
    "    \n",
    "    ftilda_verified_users=[]\n",
    "    def verified_users():\n",
    "        list_verified_users = []\n",
    "        begin_time=sorted_List_time[0]\n",
    "\n",
    "        tweet_number=0\n",
    "        tweet_info=0\n",
    "        interval =1\n",
    "        for i in range (1,51):\n",
    "\n",
    "            end_time = begin_time+timedelta(hours=interval_duration)\n",
    "\n",
    "            if (tweet_info<len(sorted_List_time) and sorted_List_time[tweet_info]<=end_time):\n",
    "                #print('Interval',interval, ':')\n",
    "                microblogs_verified_users = 0\n",
    "                total_microblogs=0\n",
    "\n",
    "                while (begin_time <= end_time and tweet_info<len(sorted_List_time)):\n",
    "\n",
    "                    if (sorted_List_time[tweet_info]<=end_time):\n",
    "                        tweet_number +=1\n",
    "                        total_microblogs +=1\n",
    "                        tweet_detail= List_sorted_users[tweet_info]\n",
    "                        required_data = tweet_detail\n",
    "\n",
    "                        if required_data== True :\n",
    "                            #print(tweet_detail)\n",
    "                            microblogs_verified_users+=1\n",
    "                        tweet_info+=1\n",
    "                    begin_time += timedelta(hours=1)\n",
    "\n",
    "\n",
    "                percent = microblogs_verified_users/total_microblogs\n",
    "                #print(\"microblogs with verified users:\",microblogs_verified_users)\n",
    "                #print(\"total number of microblogs:\",total_microblogs)\n",
    "                percentage = round(percent * 100,2)\n",
    "                #print(\"percentage of microblogs with verified users:\",percentage ,\"%\")\n",
    "                list_verified_users.append(percentage)\n",
    "                #print (\"\\n\")\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "            else:\n",
    "                #print('Interval',interval, ': No tweets')\n",
    "                list_verified_users.append(0)\n",
    "                interval+=1\n",
    "                end_time += timedelta(hours=1)\n",
    "                begin_time = end_time\n",
    "        std_dev=np.std(list_verified_users)\n",
    "        mean_f=np.mean(list_verified_users)\n",
    "        length_f = len(list_verified_users)\n",
    "        for i in range(length_f):\n",
    "            f_tilda= (list_verified_users[i]- mean_f)/std_dev\n",
    "            ftilda_verified_users.append(f_tilda)\n",
    "\n",
    "    verified_users()\n",
    "    filelist.extend(ftilda_verified_users)\n",
    "    \n",
    "     ##************Average # of friends of users***********************###\n",
    "    \n",
    "    ftilda_users_friends = []\n",
    "    def users_friends():\n",
    "        list_users_friends = []\n",
    "        begin_time=sorted_List_time[0]\n",
    "\n",
    "        tweet_number=0\n",
    "        tweet_info=0\n",
    "        interval =1\n",
    "        for i in range (1,51):\n",
    "\n",
    "            end_time = begin_time+timedelta(hours=interval_duration)\n",
    "\n",
    "            if (tweet_info<len(sorted_List_time) and sorted_List_time[tweet_info]<=end_time):\n",
    "                #print('Interval',interval, ':')\n",
    "                count=0\n",
    "                sum_ =0\n",
    "                microblogs_verified_users = 0\n",
    "                total_microblogs=0\n",
    "\n",
    "                while (begin_time <= end_time and tweet_info<len(sorted_List_time)):\n",
    "                    #print('before if z=',z)\n",
    "\n",
    "                    if (sorted_List_time[tweet_info]<=end_time):\n",
    "                        tweet_number +=1\n",
    "                        count +=1\n",
    "                        #print ('Tweet_',tweet_number,':','|||',List_sorted_friends[tweet_info])\n",
    "                        sum_ = sum_+(List_sorted_friends[tweet_info])\n",
    "                        tweet_info+=1\n",
    "                    begin_time += timedelta(hours=1)\n",
    "\n",
    "                total_count = round(sum_/count,2)\n",
    "                #print (\"Average # of friends of user\",total_count)\n",
    "                #print (\"\\n\")\n",
    "                list_users_friends.append(total_count)\n",
    "                begin_time = end_time\n",
    "                interval +=1\n",
    "            else:\n",
    "                #print('Interval',interval, ': No tweets')\n",
    "                list_users_friends.append(0)\n",
    "                interval+=1\n",
    "                end_time += timedelta(hours=1)\n",
    "                begin_time = end_time\n",
    "        std_dev=np.std(list_users_friends)\n",
    "        mean_f=np.mean(list_users_friends)\n",
    "        length_f = len(list_users_friends)\n",
    "        for i in range(length_f):\n",
    "            f_tilda= (list_users_friends[i]- mean_f)/std_dev\n",
    "            ftilda_users_friends.append(f_tilda)  \n",
    "\n",
    "    users_friends()\n",
    "    filelist.extend(ftilda_users_friends)\n",
    "    \n",
    "     ######****** Average # of followers of users ****************###\n",
    "    \n",
    "    ftilda_users_followers = []\n",
    "    def users_followers():\n",
    "        list_users_followers = []\n",
    "        begin_time=sorted_List_time[0]\n",
    "\n",
    "        tweet_number=0\n",
    "        tweet_info=0\n",
    "        interval =1\n",
    "        for i in range (1,51):\n",
    "\n",
    "            end_time = begin_time+timedelta(hours=interval_duration)\n",
    "\n",
    "            if (tweet_info<len(sorted_List_time) and sorted_List_time[tweet_info]<=end_time):\n",
    "                #print('Interval',interval, ':')\n",
    "                count=0\n",
    "                sum_ =0\n",
    "\n",
    "                while (begin_time <= end_time and tweet_info<len(sorted_List_time)):\n",
    "                    #print('before if z=',z)\n",
    "\n",
    "                    if (sorted_List_time[tweet_info]<=end_time):\n",
    "                        tweet_number +=1\n",
    "                        count +=1\n",
    "                        #print ('Tweet_',tweet_number,':','|||',List_sorted_followers[tweet_info])\n",
    "                        sum_ = sum_+(List_sorted_followers[tweet_info])\n",
    "                        tweet_info+=1\n",
    "                    begin_time += timedelta(hours=1)\n",
    "\n",
    "                total_count = round(sum_/count,2)\n",
    "                #print (\"Average # of followers of user\",total_count)\n",
    "                list_users_followers.append(total_count)\n",
    "                #print (\"\\n\")\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "            else:\n",
    "                #print('Interval',interval, ': No tweets')\n",
    "                list_users_followers.append(0)\n",
    "                end_time += timedelta(hours=1)\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "        std_dev=np.std(list_users_followers)\n",
    "        mean_f=np.mean(list_users_followers)\n",
    "        length_f = len(list_users_followers)\n",
    "        for i in range(length_f):\n",
    "            f_tilda= (list_users_followers[i]- mean_f)/std_dev\n",
    "            ftilda_users_followers.append(f_tilda)\n",
    "         \n",
    "    users_followers()\n",
    "    filelist.extend(ftilda_users_followers)\n",
    "    \n",
    "    \n",
    "     ###*********** Average # of posts of users    *****************####\n",
    "    ftilda_users_posts = []\n",
    "    def users_posts():\n",
    "\n",
    "        list_users_posts=[]\n",
    "        begin_time=sorted_List_time[0]\n",
    "        tweet_number=0\n",
    "        tweet_info=0\n",
    "        interval =1\n",
    "        for i in range (1,51):\n",
    "\n",
    "            end_time = begin_time+timedelta(hours=interval_duration)\n",
    "\n",
    "            if (tweet_info<len(sorted_List_time) and sorted_List_time[tweet_info]<=end_time):\n",
    "                #print('Interval',interval, ':')\n",
    "                count=0\n",
    "                sum_ =0\n",
    "\n",
    "\n",
    "                while (begin_time <= end_time and tweet_info<len(sorted_List_time)):\n",
    "                    #print('before if z=',z)\n",
    "\n",
    "                    if (sorted_List_time[tweet_info]<=end_time):\n",
    "                        tweet_number +=1\n",
    "                        count +=1\n",
    "                        #print ('Tweet_',tweet_number,':','|||',List_sorted_posts[tweet_info])\n",
    "                        sum_ = sum_+(List_sorted_posts[tweet_info])\n",
    "                        tweet_info+=1\n",
    "                    begin_time += timedelta(hours=1)\n",
    "\n",
    "                avg_user_posts = round(sum_/count,2)\n",
    "                #print (\"Average # of posts of user\",avg_user_posts)\n",
    "                list_users_posts.append(avg_user_posts)\n",
    "                #print (\"\\n\")\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "            else:\n",
    "                #print('Interval',interval, ': No tweets')\n",
    "                list_users_posts.append(0)\n",
    "                end_time += timedelta(hours=1)\n",
    "                begin_time = end_time\n",
    "                interval+=1\n",
    "        std_dev=np.std(list_users_posts)\n",
    "        mean_f=np.mean(list_users_posts)\n",
    "        length_f = len(list_users_posts)\n",
    "        for i in range(length_f):\n",
    "            f_tilda= (list_users_posts[i]- mean_f)/std_dev\n",
    "            ftilda_users_posts.append(f_tilda)\n",
    "            \n",
    "    users_posts()\n",
    "    filelist.extend(ftilda_users_posts)\n",
    "   ################################################# \n",
    "\n",
    "#All the svector[] lists contain the S vector which is equation (5)  S D(i,t) = (FD(i,t+1) -FD i,t ) / Interval(Ei)\n",
    "\n",
    "    svector_avg_microblogs_length = [(x - ftilda_avg_microblogs_length[i - 1])/interval_duration for i, x in enumerate(ftilda_avg_microblogs_length)][1:]\n",
    "    svector_positive_negative_words = [(x - ftilda_positive_negative_words[i - 1])/interval_duration for i, x in enumerate(ftilda_positive_negative_words)][1:]\n",
    "    svector_microblosg_url = [(x - ftilda_microblosg_url[i - 1])/interval_duration for i, x in enumerate(ftilda_microblosg_url)][1:]\n",
    "    svector_microblogs_emoticons = [(x - ftilda_microblogs_emoticons[i - 1])/interval_duration for i, x in enumerate(ftilda_microblogs_emoticons)][1:]\n",
    "    svector_positive_negaive_microblogs = [(x - ftilda_positive_negaive_microblogs[i - 1])/interval_duration for i, x in enumerate(ftilda_positive_negaive_microblogs)][1:]\n",
    "    svector_first_person_pronouns = [(x - ftilda_first_person_pronouns[i - 1])/interval_duration for i, x in enumerate(ftilda_first_person_pronouns)][1:]\n",
    "    svector_microblogs_with_hashtags = [(x - ftilda_microblogs_with_hashtags[i - 1])/interval_duration for i, x in enumerate(ftilda_microblogs_with_hashtags)][1:]\n",
    "    svector_microblogs_with_at = [(x - ftilda_microblogs_with_at[i - 1])/interval_duration for i, x in enumerate(ftilda_microblogs_with_at)][1:]\n",
    "    svector_microblogs_with_question = [(x - ftilda_microblogs_with_question[i - 1])/interval_duration for i, x in enumerate(ftilda_microblogs_with_question)][1:]\n",
    "    svector_microblogs_with_exclamation = [(x - ftilda_microblogs_with_exclamation[i - 1])/interval_duration for i, x in enumerate(ftilda_microblogs_with_exclamation)][1:]\n",
    "    svector_microblogs_with_question_exclamation = [(x - ftilda_microblogs_with_question_exclamation[i - 1])/interval_duration for i, x in enumerate(ftilda_microblogs_with_question_exclamation)][1:]\n",
    "    svector_verified_users = [(x - ftilda_verified_users[i - 1])/interval_duration for i, x in enumerate(ftilda_verified_users)][1:]\n",
    "    svector_users_friends = [(x - ftilda_users_friends[i - 1])/interval_duration for i, x in enumerate(ftilda_users_friends)][1:]\n",
    "    svector_users_followers = [(x - ftilda_users_followers[i - 1])/interval_duration for i, x in enumerate(ftilda_users_followers)][1:]\n",
    "    svector_users_posts = [(x - ftilda_users_posts[i - 1])/interval_duration for i, x in enumerate(ftilda_users_posts)][1:]\n",
    "    \n",
    "    filelist.extend(svector_avg_microblogs_length)\n",
    "    filelist.extend(svector_positive_negative_words)\n",
    "    filelist.extend(svector_microblosg_url)\n",
    "    filelist.extend(svector_microblogs_emoticons)\n",
    "    filelist.extend(svector_positive_negaive_microblogs)\n",
    "    filelist.extend(svector_first_person_pronouns)\n",
    "    filelist.extend(svector_microblogs_with_hashtags)\n",
    "    filelist.extend(svector_microblogs_with_at)\n",
    "    filelist.extend(svector_microblogs_with_question)\n",
    "    filelist.extend(svector_microblogs_with_exclamation)\n",
    "    filelist.extend(svector_microblogs_with_question_exclamation)\n",
    "    filelist.extend(svector_verified_users)\n",
    "    filelist.extend(svector_users_friends)\n",
    "    filelist.extend(svector_users_followers)\n",
    "    filelist.extend(svector_users_posts)\n",
    "    \n",
    "    # the final value which we have to predict i.e. if a tweet is a rumor or not    \n",
    "    rumorlabel =data['rumor_label']\n",
    "    if rumorlabel is False:\n",
    "        falselist=[0]\n",
    "        filelist.extend(falselist)\n",
    "        falselist.clear()\n",
    "    elif rumorlabel is True:\n",
    "        truelist= [1]\n",
    "        filelist.extend(truelist)\n",
    "        truelist.clear()\n",
    "\n",
    "    \n",
    "    dataset_list.append(filelist) # appending data to of each event to the main list\n",
    "\n",
    "    ########################## the main() function ############################################\n",
    "\"\"\"           \n",
    "if __name__ == '__main__':\n",
    "    average_length_posts_of_user()\n",
    "    positive_negative_words()\n",
    "    microblogs_with_url()\n",
    "    microblogs_with_emoticons()\n",
    "    positive_negaive_microblogs()\n",
    "    first_person_pronouns()\n",
    "    microblogs_with_hashtags()\n",
    "    microblogs_with_at()\n",
    "    microblogs_with_question()\n",
    "    microblogs_with_exclamation()\n",
    "    microblogs_with_question_exclamation()\n",
    "    verified_users()\n",
    "    users_friends()\n",
    "    users_followers()\n",
    "    users_posts()\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasetfinal = np.array(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444, 1486)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetfinal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(datasetfinal) # to shuffle our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###********* TO write the data in a CSV file *********** we do this so as to avoid the time extracting the data from the original files(JSON files here)\n",
    "#np.savetxt(\"rumor_detection.csv\", datasetfinal, delimiter=\",\")\n",
    "# open the csv file manually, insert a row on the top and  add something like 'xyz' on the first row first coloumn because pd.read_csv consider the first row as heading and ignores it\n",
    "#datafile = pd.read_csv(\"rumor_detection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#datafile.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X, y = datafile.iloc[:, :-1], datafile.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = datasetfinal[:, :-1], datasetfinal[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444, 1485)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#taking care of missing data\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "X  = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=5, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "kf.get_n_splits(X)\n",
    "print(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106\n",
      " 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124\n",
      " 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142\n",
      " 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160\n",
      " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178\n",
      " 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196\n",
      " 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214\n",
      " 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232\n",
      " 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250\n",
      " 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268\n",
      " 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286\n",
      " 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304\n",
      " 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322\n",
      " 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358\n",
      " 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376\n",
      " 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394\n",
      " 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412\n",
      " 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430\n",
      " 431 432 433 434 435 436 437 438 439 440 441 442 443] TEST: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88 178\n",
      " 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196\n",
      " 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214\n",
      " 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232\n",
      " 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250\n",
      " 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268\n",
      " 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286\n",
      " 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304\n",
      " 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322\n",
      " 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358\n",
      " 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376\n",
      " 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394\n",
      " 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412\n",
      " 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430\n",
      " 431 432 433 434 435 436 437 438 439 440 441 442 443] TEST: [ 89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106\n",
      " 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124\n",
      " 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142\n",
      " 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160\n",
      " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 267 268\n",
      " 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286\n",
      " 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304\n",
      " 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322\n",
      " 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358\n",
      " 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376\n",
      " 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394\n",
      " 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412\n",
      " 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430\n",
      " 431 432 433 434 435 436 437 438 439 440 441 442 443] TEST: [178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195\n",
      " 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213\n",
      " 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231\n",
      " 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249\n",
      " 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 356 357 358\n",
      " 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376\n",
      " 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394\n",
      " 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412\n",
      " 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430\n",
      " 431 432 433 434 435 436 437 438 439 440 441 442 443] TEST: [267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284\n",
      " 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302\n",
      " 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320\n",
      " 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338\n",
      " 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355]\n",
      "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355] TEST: [356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373\n",
      " 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391\n",
      " 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409\n",
      " 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427\n",
      " 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443]\n"
     ]
    }
   ],
   "source": [
    "# classifying the data as train and test\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if not k-fold\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356, 1474)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88, 1474)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(356,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementing linear SVM to our dataset\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel='linear', random_state=0)\n",
    "classifier.fit(X_train, y_train)  # training our train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predicting the values on the test dataset\n",
    "y_pred =classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.9772727272727273\n",
      "Test Precision:  0.9883720930232558\n",
      "Recall:  0.75\n",
      "F1 Score:  0.8274509803921568\n"
     ]
    }
   ],
   "source": [
    "# printing out the results to check the accuracy of the model\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, mean_squared_error,classification_report, confusion_matrix\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "#print('Test Accuracy: ',accuracy_score(y_test, y_pred, normalize=False) )\n",
    "print('Test Accuracy: ',accuracy_score(y_test, y_pred) )\n",
    "\n",
    "print('Test Precision: ',precision_score(y_test, y_pred, average=\"macro\"))\n",
    "#print('Test Precision: ',precision_score(y_test, y_pred))\n",
    "\n",
    "print('Recall: ',recall_score(y_test, y_pred, average=\"macro\")) \n",
    "#print('Recall: ',recall_score(y_test, y_pred)) \n",
    "\n",
    "print('F1 Score: ',f1_score(y_test, y_pred, average=\"macro\"))\n",
    "#print('F1 Score: ',f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 =  0.9555555555555555\n",
      "mean squared error =  0.022727272727272728\n"
     ]
    }
   ],
   "source": [
    "scorevalues = cross_val_score(classifier, X_test, y_test, cv=5)\n",
    "print (\"R^2 = \", scorevalues.mean())\n",
    "print(\"mean squared error = \",mean_squared_error(y_test, y_pred).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[84  0]\n",
      " [ 2  2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankit\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\ankit\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\ankit\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\ankit\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is second one\n",
      "[-0.03370787 -0.04494382 -0.01123596 -0.01123596 -0.02272727]\n",
      "this is root mean squares \n",
      "[nan nan nan nan nan]\n",
      "this is   average root mean squares \n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankit\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation \n",
    "s = cross_validation.KFold(len(X), n_folds=5, shuffle=True, random_state=42)\n",
    "scores = cross_validation.cross_val_score(classifier, X, y, cv=s, scoring='mean_squared_error')\n",
    "print('this is second one')\n",
    "print (scores)\n",
    "scores1= -scores\n",
    "rmse_scores = np.sqrt(scores1)\n",
    "print('this is root mean squares ')\n",
    "print(rmse_scores)\n",
    "print('this is   average root mean squares ')\n",
    "print(rmse_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
